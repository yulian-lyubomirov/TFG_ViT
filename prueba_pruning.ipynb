{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from train_test import train_kd_pruning,train_pruning,test\n",
    "from helpers import get_data_loader\n",
    "from model import ViT\n",
    "from helpers import load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "train_loader, test_loader = get_data_loader(\n",
    "    3000, 2, \"datasets/cifar-100/cifar-100-python\", download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = ViT(\n",
    "    image_size=32,\n",
    "    patch_size=4,\n",
    "    num_classes=100,\n",
    "    dim=768,\n",
    "    depth=7,\n",
    "    heads=12,\n",
    "    mlp_dim=512,\n",
    "    dropout=0.1,\n",
    ").to(\"cuda\")\n",
    "\n",
    "teacher_save_path = \"save_model/cifar-100/vit_16_teacher_cifar-100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "student_kd_save_path = \"save_model/cifar-100/vit_16_student_kd_cifar-100\"\n",
    "student_kd = ViT(\n",
    "    image_size=32,\n",
    "    patch_size=4,\n",
    "    num_classes=100,\n",
    "    dim=768,\n",
    "    depth=6,\n",
    "    heads=6,\n",
    "    mlp_dim=512,\n",
    "    dropout=0.1,\n",
    ").to(\"cuda\")\n",
    "\n",
    "teacher_load_path = f\"{teacher_save_path}/best_model.pt\"\n",
    "\n",
    "train_kd_pruning(\n",
    "    student_kd,\n",
    "    teacher_model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    T=2,\n",
    "    soft_target_loss_weight=0.3,\n",
    "    ce_loss_weight=0.7,\n",
    "    epochs=100,\n",
    "    learning_rate=0.0001,\n",
    "    device='cuda',\n",
    "    save_path=student_kd_save_path,\n",
    "    load_path_teacher=teacher_load_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint(\n",
    "    student_kd, \"save_model/cifar-100/vit_16_student_kd_cifar-100/best_model.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 4.325147797079647\n",
      "Current Learning Rate: 9.997532801828658e-05\n",
      "Test Accuracy: 9.11%\n",
      "max_test_accuracy : 9.11\n",
      "Epoch 2/100, Loss: 3.8460660401512596\n",
      "Current Learning Rate: 9.990133642141359e-05\n",
      "Test Accuracy: 14.90%\n",
      "max_test_accuracy : 14.9\n",
      "Epoch 3/100, Loss: 3.5764001537771786\n",
      "Current Learning Rate: 9.977809823015401e-05\n",
      "Test Accuracy: 17.90%\n",
      "max_test_accuracy : 17.9\n",
      "Epoch 4/100, Loss: 3.3905582007239845\n",
      "Current Learning Rate: 9.960573506572391e-05\n",
      "Test Accuracy: 20.53%\n",
      "max_test_accuracy : 20.53\n",
      "Epoch 5/100, Loss: 3.232750654220581\n",
      "Current Learning Rate: 9.93844170297569e-05\n",
      "Test Accuracy: 22.00%\n",
      "max_test_accuracy : 22.0\n",
      "Epoch 6/100, Loss: 3.089136235854205\n",
      "Current Learning Rate: 9.911436253643445e-05\n",
      "Test Accuracy: 24.54%\n",
      "max_test_accuracy : 24.54\n",
      "Epoch 7/100, Loss: 2.9554501982296215\n",
      "Current Learning Rate: 9.879583809693738e-05\n",
      "Test Accuracy: 26.11%\n",
      "max_test_accuracy : 26.11\n",
      "Epoch 8/100, Loss: 2.839271910050336\n",
      "Current Learning Rate: 9.842915805643157e-05\n",
      "Test Accuracy: 27.36%\n",
      "max_test_accuracy : 27.36\n",
      "Epoch 9/100, Loss: 2.734014833674711\n",
      "Current Learning Rate: 9.801468428384717e-05\n",
      "Test Accuracy: 28.40%\n",
      "max_test_accuracy : 28.4\n",
      "Epoch 10/100, Loss: 2.626436205471263\n",
      "Current Learning Rate: 9.75528258147577e-05\n",
      "Test Accuracy: 29.35%\n",
      "max_test_accuracy : 29.35\n",
      "Epoch 11/100, Loss: 2.515779635485481\n",
      "Current Learning Rate: 9.70440384477113e-05\n",
      "Test Accuracy: 30.06%\n",
      "max_test_accuracy : 30.06\n",
      "Epoch 12/100, Loss: 2.4275797254898968\n",
      "Current Learning Rate: 9.64888242944126e-05\n",
      "Test Accuracy: 30.62%\n",
      "max_test_accuracy : 30.62\n",
      "Epoch 13/100, Loss: 2.3209435378803924\n",
      "Current Learning Rate: 9.588773128419907e-05\n",
      "Test Accuracy: 31.42%\n",
      "max_test_accuracy : 31.42\n",
      "Epoch 14/100, Loss: 2.2269669701071346\n",
      "Current Learning Rate: 9.5241352623301e-05\n",
      "Test Accuracy: 31.53%\n",
      "max_test_accuracy : 31.53\n",
      "Epoch 15/100, Loss: 2.125780400107889\n",
      "Current Learning Rate: 9.455032620941843e-05\n",
      "Test Accuracy: 32.10%\n",
      "max_test_accuracy : 32.1\n",
      "Epoch 16/100, Loss: 2.0179789977915146\n",
      "Current Learning Rate: 9.381533400219321e-05\n",
      "Test Accuracy: 31.99%\n",
      "Epoch 17/100, Loss: 1.9074443718966316\n",
      "Current Learning Rate: 9.303710135019722e-05\n",
      "Test Accuracy: 32.93%\n",
      "max_test_accuracy : 32.93\n",
      "Epoch 18/100, Loss: 1.8008663724450504\n",
      "Current Learning Rate: 9.22163962751008e-05\n",
      "Test Accuracy: 33.28%\n",
      "max_test_accuracy : 33.28\n",
      "Epoch 19/100, Loss: 1.6903630284702076\n",
      "Current Learning Rate: 9.135402871372814e-05\n",
      "Test Accuracy: 32.75%\n",
      "Epoch 20/100, Loss: 1.5672184088650871\n",
      "Current Learning Rate: 9.045084971874742e-05\n",
      "Test Accuracy: 32.83%\n",
      "Epoch 21/100, Loss: 1.4565060769810396\n",
      "Current Learning Rate: 8.950775061878456e-05\n",
      "Test Accuracy: 32.49%\n",
      "Epoch 22/100, Loss: 1.3401482946732466\n",
      "Current Learning Rate: 8.852566213878951e-05\n",
      "Test Accuracy: 33.05%\n",
      "Epoch 23/100, Loss: 1.231351845404681\n",
      "Current Learning Rate: 8.750555348152303e-05\n",
      "Test Accuracy: 32.57%\n",
      "Epoch 24/100, Loss: 1.105176652179045\n",
      "Current Learning Rate: 8.644843137107063e-05\n",
      "Test Accuracy: 32.56%\n",
      "Epoch 25/100, Loss: 0.9898804741747239\n",
      "Current Learning Rate: 8.535533905932742e-05\n",
      "Test Accuracy: 32.42%\n",
      "Epoch 26/100, Loss: 0.882680545834934\n",
      "Current Learning Rate: 8.422735529643448e-05\n",
      "Test Accuracy: 31.92%\n",
      "Epoch 27/100, Loss: 0.7803751195178312\n",
      "Current Learning Rate: 8.306559326618263e-05\n",
      "Test Accuracy: 31.77%\n",
      "Epoch 28/100, Loss: 0.6892552375793457\n",
      "Current Learning Rate: 8.187119948743452e-05\n",
      "Test Accuracy: 31.91%\n",
      "Epoch 29/100, Loss: 0.5943892247536603\n",
      "Current Learning Rate: 8.064535268264887e-05\n",
      "Test Accuracy: 32.09%\n",
      "Epoch 30/100, Loss: 0.5075383326586556\n",
      "Current Learning Rate: 7.938926261462371e-05\n",
      "Test Accuracy: 31.35%\n",
      "Epoch 31/100, Loss: 0.4397438045810251\n",
      "Current Learning Rate: 7.810416889260659e-05\n",
      "Test Accuracy: 31.59%\n",
      "Epoch 32/100, Loss: 0.37580147911520567\n",
      "Current Learning Rate: 7.679133974894988e-05\n",
      "Test Accuracy: 31.46%\n",
      "Epoch 33/100, Loss: 0.31725624028374166\n",
      "Current Learning Rate: 7.545207078751862e-05\n",
      "Test Accuracy: 31.47%\n",
      "Epoch 34/100, Loss: 0.26876161203664894\n",
      "Current Learning Rate: 7.408768370508582e-05\n",
      "Test Accuracy: 31.51%\n",
      "Epoch 35/100, Loss: 0.22604170266319723\n",
      "Current Learning Rate: 7.26995249869774e-05\n",
      "Test Accuracy: 31.55%\n",
      "Epoch 36/100, Loss: 0.1924794579253477\n",
      "Current Learning Rate: 7.12889645782537e-05\n",
      "Test Accuracy: 31.44%\n",
      "Epoch 37/100, Loss: 0.16310188174247742\n",
      "Current Learning Rate: 6.985739453173908e-05\n",
      "Test Accuracy: 31.31%\n",
      "Epoch 38/100, Loss: 0.13652291893959045\n",
      "Current Learning Rate: 6.840622763423397e-05\n",
      "Test Accuracy: 31.27%\n",
      "Epoch 39/100, Loss: 0.11638711305225596\n",
      "Current Learning Rate: 6.693689601226464e-05\n",
      "Test Accuracy: 31.46%\n",
      "Epoch 40/100, Loss: 0.10125043900573955\n",
      "Current Learning Rate: 6.545084971874744e-05\n",
      "Test Accuracy: 31.18%\n",
      "Epoch 41/100, Loss: 0.08735731024952496\n",
      "Current Learning Rate: 6.394955530196154e-05\n",
      "Test Accuracy: 31.28%\n",
      "Epoch 42/100, Loss: 0.07459085916771609\n",
      "Current Learning Rate: 6.24344943582428e-05\n",
      "Test Accuracy: 31.24%\n",
      "Epoch 43/100, Loss: 0.06540561226360939\n",
      "Current Learning Rate: 6.0907162069827195e-05\n",
      "Test Accuracy: 31.54%\n",
      "Epoch 44/100, Loss: 0.05882165800122654\n",
      "Current Learning Rate: 5.93690657292863e-05\n",
      "Test Accuracy: 31.25%\n",
      "Epoch 45/100, Loss: 0.051851948832764345\n",
      "Current Learning Rate: 5.7821723252011606e-05\n",
      "Test Accuracy: 31.28%\n",
      "Epoch 46/100, Loss: 0.04694699605598169\n",
      "Current Learning Rate: 5.6266661678215284e-05\n",
      "Test Accuracy: 31.09%\n",
      "Epoch 47/100, Loss: 0.04211464042172713\n",
      "Current Learning Rate: 5.470541566592577e-05\n",
      "Test Accuracy: 31.12%\n",
      "Epoch 48/100, Loss: 0.03922383750186247\n",
      "Current Learning Rate: 5.313952597646573e-05\n",
      "Test Accuracy: 31.04%\n",
      "Epoch 49/100, Loss: 0.03564648374038584\n",
      "Current Learning Rate: 5.1570537953906474e-05\n",
      "Test Accuracy: 31.06%\n",
      "Epoch 50/100, Loss: 0.03284886829993304\n",
      "Current Learning Rate: 5.0000000000000057e-05\n",
      "Test Accuracy: 31.06%\n",
      "Epoch 51/100, Loss: 0.030370290226796093\n",
      "Current Learning Rate: 4.842946204609364e-05\n",
      "Test Accuracy: 31.04%\n",
      "Epoch 52/100, Loss: 3.7745893843033733\n",
      "Current Learning Rate: 4.686047402353438e-05\n",
      "Test Accuracy: 20.40%\n",
      "Epoch 53/100, Loss: 2.73434115858639\n",
      "Current Learning Rate: 4.529458433407433e-05\n",
      "Test Accuracy: 25.39%\n",
      "Epoch 54/100, Loss: 2.0792677332373226\n",
      "Current Learning Rate: 4.3733338321784836e-05\n",
      "Test Accuracy: 26.75%\n",
      "Epoch 55/100, Loss: 1.6080639502581429\n",
      "Current Learning Rate: 4.21782767479885e-05\n",
      "Test Accuracy: 26.98%\n",
      "Epoch 56/100, Loss: 1.3281083527733297\n",
      "Current Learning Rate: 4.0630934270713814e-05\n",
      "Test Accuracy: 26.76%\n",
      "Epoch 57/100, Loss: 1.134387444047367\n",
      "Current Learning Rate: 3.909283793017293e-05\n",
      "Test Accuracy: 27.06%\n",
      "Epoch 58/100, Loss: 0.9960906751015607\n",
      "Current Learning Rate: 3.75655056417573e-05\n",
      "Test Accuracy: 27.10%\n",
      "Epoch 59/100, Loss: 0.893759219085469\n",
      "Current Learning Rate: 3.605044469803859e-05\n",
      "Test Accuracy: 27.35%\n",
      "Epoch 60/100, Loss: 0.8090355676763198\n",
      "Current Learning Rate: 3.454915028125269e-05\n",
      "Test Accuracy: 27.00%\n",
      "Epoch 61/100, Loss: 0.7340841223211849\n",
      "Current Learning Rate: 3.3063103987735474e-05\n",
      "Test Accuracy: 26.94%\n",
      "Epoch 62/100, Loss: 0.6754307817010319\n",
      "Current Learning Rate: 3.159377236576616e-05\n",
      "Test Accuracy: 26.83%\n",
      "Epoch 63/100, Loss: 0.6310697829022127\n",
      "Current Learning Rate: 3.0142605468261005e-05\n",
      "Test Accuracy: 26.95%\n",
      "Epoch 64/100, Loss: 0.592012296704685\n",
      "Current Learning Rate: 2.8711035421746404e-05\n",
      "Test Accuracy: 26.82%\n",
      "Epoch 65/100, Loss: 0.5592097324483535\n",
      "Current Learning Rate: 2.73004750130227e-05\n",
      "Test Accuracy: 26.57%\n",
      "Epoch 66/100, Loss: 0.5279744022032794\n",
      "Current Learning Rate: 2.5912316294914263e-05\n",
      "Test Accuracy: 26.76%\n",
      "Epoch 67/100, Loss: 0.5039502014132107\n",
      "Current Learning Rate: 2.4547929212481466e-05\n",
      "Test Accuracy: 26.73%\n",
      "Epoch 68/100, Loss: 0.48373249523779926\n",
      "Current Learning Rate: 2.320866025105019e-05\n",
      "Test Accuracy: 26.93%\n",
      "Epoch 69/100, Loss: 0.46498543900602\n",
      "Current Learning Rate: 2.1895831107393498e-05\n",
      "Test Accuracy: 26.39%\n",
      "Epoch 70/100, Loss: 0.4488935155027053\n",
      "Current Learning Rate: 2.061073738537638e-05\n",
      "Test Accuracy: 26.60%\n",
      "Epoch 71/100, Loss: 0.43721206924494577\n",
      "Current Learning Rate: 1.9354647317351198e-05\n",
      "Test Accuracy: 26.56%\n",
      "Epoch 72/100, Loss: 0.4233072084539077\n",
      "Current Learning Rate: 1.812880051256554e-05\n",
      "Test Accuracy: 26.61%\n",
      "Epoch 73/100, Loss: 0.4106665797093335\n",
      "Current Learning Rate: 1.693440673381744e-05\n",
      "Test Accuracy: 26.49%\n",
      "Epoch 74/100, Loss: 0.4028817047091091\n",
      "Current Learning Rate: 1.577264470356559e-05\n",
      "Test Accuracy: 26.64%\n",
      "Epoch 75/100, Loss: 0.3930145887767567\n",
      "Current Learning Rate: 1.464466094067265e-05\n",
      "Test Accuracy: 26.53%\n",
      "Epoch 76/100, Loss: 0.38696468402357664\n",
      "Current Learning Rate: 1.3551568628929456e-05\n",
      "Test Accuracy: 26.43%\n",
      "Epoch 77/100, Loss: 0.38018013624583974\n",
      "Current Learning Rate: 1.249444651847704e-05\n",
      "Test Accuracy: 26.44%\n",
      "Epoch 78/100, Loss: 0.3747622002573574\n",
      "Current Learning Rate: 1.1474337861210562e-05\n",
      "Test Accuracy: 26.42%\n",
      "Epoch 79/100, Loss: 0.370438030537437\n",
      "Current Learning Rate: 1.0492249381215497e-05\n",
      "Test Accuracy: 26.59%\n",
      "Epoch 80/100, Loss: 0.3651394353193395\n",
      "Current Learning Rate: 9.549150281252648e-06\n",
      "Test Accuracy: 26.25%\n",
      "Epoch 81/100, Loss: 0.3600880328346701\n",
      "Current Learning Rate: 8.645971286271927e-06\n",
      "Test Accuracy: 26.52%\n",
      "Epoch 82/100, Loss: 4.332387685775757\n",
      "Current Learning Rate: 7.78360372489927e-06\n",
      "Test Accuracy: 12.62%\n",
      "Epoch 83/100, Loss: 3.595131453345804\n",
      "Current Learning Rate: 6.962898649802823e-06\n",
      "Test Accuracy: 16.78%\n",
      "Epoch 84/100, Loss: 3.3782476116629208\n",
      "Current Learning Rate: 6.184665997806831e-06\n",
      "Test Accuracy: 18.14%\n",
      "Epoch 85/100, Loss: 3.236050619798548\n",
      "Current Learning Rate: 5.449673790581619e-06\n",
      "Test Accuracy: 19.29%\n",
      "Epoch 86/100, Loss: 3.129679623772116\n",
      "Current Learning Rate: 4.75864737669904e-06\n",
      "Test Accuracy: 20.17%\n",
      "Epoch 87/100, Loss: 3.0423626478980568\n",
      "Current Learning Rate: 4.112268715800961e-06\n",
      "Test Accuracy: 20.32%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_pruning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpruning_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstructured\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# load_path=teacher_load_path\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/TFG2/train_test.py:343\u001b[0m, in \u001b[0;36mtrain_pruning\u001b[0;34m(model, train_loader, test_loader, epochs, learning_rate, device, pruning_method, save_path, load_path)\u001b[0m\n\u001b[1;32m    341\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    342\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 343\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    346\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:452\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    450\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 452\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:349\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    343\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    348\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    350\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:349\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    343\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    348\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    350\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_pruning(\n",
    "    teacher_model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=100,\n",
    "    learning_rate=0.0001,\n",
    "    device=\"cuda\",\n",
    "    save_path=teacher_save_path,\n",
    "    pruning_method='structured'\n",
    "    # load_path=teacher_load_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 20.71%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.71"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(teacher_model, test_loader, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 20.71%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.71"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_checkpoint(\n",
    "    teacher_model, \"save_model/cifar-100/vit_16_teacher_cifar-100/best_model.pt\"\n",
    ")\n",
    "test(teacher_model, test_loader, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
